'''

Author: Daman Arora, 12th June, 2021

Computing the value function under a given policy 
can be done by multiple rollouts generated by the 
policy multiple times. The first time a state s is
visited, the cumulative discounted reward is stored.
The average of this reward over multiple runs is the 
approximate value function for the state.

'''

from blackjack import BlackjackEnv
import random
import numpy as np

class BlackjackAgent():
    
    def __init__(self):
        pass
    
    def policy(self, state):
        agent_sum, _, _ = state
        if agent_sum >= 18:
            return 0
        else:
            return 1
        
    def win_rate(self, num_rollouts=1000):

        env = BlackjackEnv()
        wins = 0
        
        for i in range(num_rollouts):
            state = env.reset()
            done = False

            while not done:
                action = self.policy(state)
                state, reward, done, _ = env.step(action)

            if reward == 1.0:
                wins += 1
                
        return wins / num_rollouts
        
def policy_evaluation(num_rollouts, agent, env, discount):
    
    values = {}
    counts = {}
    
    for i in range(num_rollouts):
    
        trajectory = []
        rewards = []

        state = env.reset()
        trajectory.append(state)

        done = False
        num_steps = 0

        while not done:
            action = agent.policy(state)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            trajectory.append(state)
            num_steps += 1

        rewards = np.array(rewards)
        rewards = rewards * np.logspace(0, num_steps - 1, num_steps, base=discount)
        cumulative_rewards = np.cumsum(rewards[::-1])[::-1]

        visited = set()
        for j, cumulative_reward in enumerate(cumulative_rewards):

            if not trajectory[j] in visited:
                visited.add(trajectory[j])
                values[trajectory[j]] = values.get(trajectory[j], 0) + cumulative_reward
                counts[trajectory[j]] = counts.get(trajectory[j], 0) + 1
                
    for key in values.keys():
        values[key] /= (counts[key])            
    
    return values

if __name__ == '__main__':
    
    agent, env = BlackjackAgent(), BlackjackEnv()
    print("Win rate:", agent.win_rate(1000))
    
    values = policy_evaluation(num_rollouts=5000, agent=agent, env=env, discount=0.99)
    print(values[(21, 2, True)], values[(19, 2, True)])